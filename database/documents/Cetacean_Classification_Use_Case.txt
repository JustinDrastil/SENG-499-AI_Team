* Created by  [Allan Rempel](    /display/~agrempel
  ), last updated by  [Ben Biffard](    /display/~bbiffard@uvic.ca
  ) on [09-Jan-23](/pages/diffpagesbyversion.action?pageId=59015744&selectedPageVersions=2&selectedPageVersions=3 "Show changes")

  4 minute read

This use case describes an adaptation of the MSc work of Kristen Kanes, which is also described in her [thesis](https://dspace.library.uvic.ca/handle/1828/9327).

The application software described in this use case is not currently publicly available; this documentation simply describes an example of how Oceans 3.0 can be and has been used.

### How it works

This code generates a filterbank of triangular filters based on the hearing capabilities of the species of interest, the Greenwood Frequency Cepstral Coefficient (GFCC) equations5,6 the procedure for calculating Mel Frequency Cepstral Coefficients4, and the sampling rate of the acoustic data for transforming the data into biologically meaningful feature-sets. For every file specified in the file list, the code then does the following:

* Downloads the file
* Calculates the start time of each acoustic data window/feature vector
* Applies a pre-emphasis filter (increases the amplitude of higher frequency signals)
* Normalizes the signal between -1 and +1
* Windows the acoustic signal read from the .wav file according to the specified window length and overlap parameters.
* Calculates the fft of each window, saving the positive half of each fft spectrum to produce a power spectral density (PSD) matrix.
* Applies the GFCC filterbank to the PSD
* Uses the DCT to calculate the GFCC feature vectors
* Calculates texture features over a window of 31 PSD windows, appending the GFCC feature vector in the middle of this 31-vector window (~1 second with default settings. The number of PSD windows may need to be changed if other settings are changed.) with said texture information (based on MARSYAS by George Tzanetakis2,3)
  + The mean and standard deviation across the texture window of:
    - centroid (the frequency at the center of the energy within each window)
    - rolloff (the frequency under which 80% of the energy in that window is contained)
    - spectral flux7
    - zero-crossings (the number of times the time-series acoustic data crosses zero within each window)
* Removes the first and last 15 feature vectors and start times from the data set (since these vectors will not have correct texture window features). This amounts to a loss of ~0.03 seconds with default settings.
* Outputs a .npz file containing the feature and time vectors for that file, with the following naming format: ICLISTENHF1251\_20130520T003321.072Z.wav.npz

The feature array and time vector are saved as an object. To extract them from the object:

* import numpy as np
* temp=np.load('ICLISTENHF1251\_20130520T003321.072Z.wav-features.npz')
* time=temp["time"]
* features=temp["features"]

### How to use the software

* Create csv file containing names of files to be classified in the following format:
  + Column A: filenames (e.g., ICLISTENHF1251\_20130520T003321.072Z.wav)
  + Column B: start times for file downloader (e.g., 2013-05-20T00:33:21.072Z)

*This will not work on filtered data. Exclude data produced during diverts (including the data with the natural filename, i.e. does not say –HPF) from analysis.*

* Replace fn=”directory/flist.csv/”
* If using an AF or LF hydrophone, replace “sr=64000” with the sampling rate of that hydrophone
* Adjust species-specific information if using to generate features for developing a classifier targeting a species other than orca
* Run the file

Species-Specific Information:

*Default values are specific to orca hearing.*

To extract features for training a classifier to classify a different species:

* Replace “windowlen=2048” and “overlap=0.5” with appropriate window length and overlap values for calculating a spectrogram to view vocalizations for that species. Windowlen must be a multiple of 2^n. Longer windows increase frequency resolution and decrease time resolution. Time resolution can be improved somewhat by increasing the window overlap (ranges from 0 to 1). Lower frequency sounds require greater frequency and time resolution than higher frequency sounds.

* Replace “texwin=31” with an appropriate, odd-numbered texture window length. The texture window should be ~0.5-1 seconds long in most cases1,2,3. Shorter texture windows may be more appropriate for higher frequency species, which have shorter vocalizations.

* Replace “sfl=600” and “sfu=114000” with the lower and upper frequency thresholds of the species of interest, respectively

* Replace “cochpos=45” with the number of cochlear positions (as determined by length in mm of the cochlea) for that species. If cochlea length measurements are not available for that species, they can be estimated using data presented in Ketten, 1997 for other species with the same cochlea type4. Cochlea length is a function of body size.

Note: If too many cochlear positions are specified for a too narrow a frequency range, the lower frequency Greenwood Frequency Cepstral Coefficients (GFCC) filters may be empty. If looking at LF animals (e.g. fin whales, blue whales, etc), check the GFCC filterbank to ensure that each filter between the lower and upper hearing thresholds of the animal contains numbers.

### References:

1. Ness, S. (2013). *The Orchive: A system for semi-automatic annotation and analysis of a large collection of bioacoustic recordings*. University of Victoria (Canada).
2. Tzanetakis, G., & Cook, P. (2002). *Manipulation, analysis and retrieval systems for audio signals*. Princeton, NJ, USA: Princeton University.
3. Tzanetakis, G. (2007). Marsyas submissions to MIREX 2007. In *Proceedings of the International Conference on Music Information Retrieval*.
4. Ketten, D. R. (1997). Structure and function in whale ears. *Bioacoustics*, *8*(1-2), 103-135.
5. Greenwood, D. D. (1990). A cochlear frequency‐position function for several species—29 years later. *The Journal of the Acoustical Society of America*, *87*(6), 2592-2605.
6. Clemins, P. J., & Johnson, M. T. (2006). Generalized perceptual linear prediction features for animal vocalization analysis. *The Journal of the Acoustical Society of America*, *120*(1), 527-534.
7. <http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/>
8. <http://jaudio.sourceforge.net/jaudio10/features/spectralflux.html>

* No labels